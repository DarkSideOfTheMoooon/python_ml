{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40a3f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ead478a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [[1 2 3 4]\n",
      " [5 6 7 8]]\n",
      "b [[1 2]\n",
      " [3 4]\n",
      " [5 6]\n",
      " [7 8]]\n"
     ]
    }
   ],
   "source": [
    "a = [[1,2,3,4],[5,6,7,8]]\n",
    "a = np.array(a)\n",
    "new_shape = (4,2)\n",
    "\n",
    "b = np.reshape(a, new_shape)\n",
    "\n",
    "print(\"a\" , a)\n",
    "print(\"b\" , b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b0cfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape(4,2):\n",
      " [[1 2]\n",
      " [3 4]\n",
      " [5 6]\n",
      " [7 8]]\n",
      "transpose:\n",
      " [[1 5]\n",
      " [2 6]\n",
      " [3 7]\n",
      " [4 8]]\n"
     ]
    }
   ],
   "source": [
    "# reshape vs transpose\n",
    "# reshape 只改变形状，不改变元素顺序；transpose 会交换轴：\n",
    "print(\"reshape(4,2):\\n\", a.reshape(4, 2))\n",
    "print(\"transpose:\\n\", a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "874308de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 1.5]\n",
      " [1.5 3.5]]\n"
     ]
    }
   ],
   "source": [
    "def is_invertible(a):\n",
    "    if len(a) != len(a[0]):\n",
    "        return false\n",
    "    return np.linalg.det(a) != 0\n",
    "\n",
    "def transform_matrix(A, T, S):\n",
    "    if not is_invertible(T) or not is_invertible(S) :\n",
    "        print(\"T or S is not invertible\")\n",
    "        return -1\n",
    "    else:\n",
    "        transformed_matrix = (np.linalg.inv(T) @ A) @ S\n",
    "    return transformed_matrix\n",
    "\n",
    "print(transform_matrix([[1, 2], [3, 4]], [[2, 0], [0, 2]], [[1, 1], [0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e8fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]]\n",
    "labels = [0, 1, 0]\n",
    "weights = [0.7, -0.4]\n",
    "bias = -0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "486670a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46257015 0.41338242 0.66818777]\n",
      "0.3349\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "prob =sigmoid(np.dot(features,weights) + bias)\n",
    "print(prob)\n",
    "mse = np.round(np.mean((labels - prob) **2), 4)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "593db0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47c0e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\n",
    "labels = [1, 0, 0]\n",
    "initial_weights = [0.1, -0.2]\n",
    "initial_bias = 0.0\n",
    "learning_rate = 0.1\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f21fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.38296168  0.33333333  0.38296168]\n",
      "[0.24445831 0.25       0.24445831]\n",
      "[-0.09361817  0.08333333  0.09361817]\n",
      "grad_w [-0.02056966 -0.29113933]\n",
      "grad_b 0.08333333333333334\n",
      "[-0.37446407  0.33748221  0.37172584]\n",
      "[0.24619359 0.24996127 0.24668354]\n",
      "[-0.09219065  0.08435748  0.09169864]\n",
      "grad_w [-0.01517434 -0.28342112]\n",
      "grad_b 0.08386547041703868\n",
      "weights [ 0.1035744  -0.14254396]\n",
      "bias -0.016719880375037202\n",
      "mse_loss [0.3033228034139421, 0.2942232621822798]\n"
     ]
    }
   ],
   "source": [
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "weights = np.array(initial_weights)\n",
    "bias = initial_bias\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "n = len(labels)\n",
    "mse_loss = []\n",
    "for epoch in range(epochs):\n",
    "    # foward\n",
    "    z = np.dot(features,weights) + bias\n",
    "    predict = sigmoid(z)\n",
    "    # mse loss\n",
    "    loss = np.mean((np.array(labels) - predict) ** 2)\n",
    "    mse_loss.append(loss)\n",
    "    #gradient descent\n",
    "    dloss_dpred = 2 * (predict - labels) / n   # n\n",
    "    dpred_dz = sigmoid_derivative(predict)     # n\n",
    "    dz_dw = features                           # n * m\n",
    "    dz_db = 1\n",
    "\n",
    "    print(dloss_dpred)\n",
    "    print(dpred_dz)\n",
    "    grad_z = dloss_dpred * dpred_dz  # n * n =  n\n",
    "    print(grad_z)\n",
    "    grad_w = np.dot(grad_z, dz_dw)  # n * m\n",
    "    print(\"grad_w\",grad_w)\n",
    "    grad_b = np.sum(grad_z) * dz_db \n",
    "    print(\"grad_b\", grad_b)\n",
    "    # update weights and bias\n",
    "    weights -= learning_rate*grad_w\n",
    "    bias -= learning_rate*grad_b\n",
    "\n",
    "print(\"weights\", weights)\n",
    "print(\"bias\", bias)\n",
    "print(\"mse_loss\", mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "012c9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "\tdef __init__(self, data, _children=(), _op=''):\n",
    "\t\tself.data = data\n",
    "\t\tself.grad = 0\n",
    "\t\tself._backward = lambda: None\n",
    "\t\tself._prev = set(_children)\n",
    "\t\tself._op = _op\n",
    "\tdef __repr__(self):\n",
    "\t\treturn f\"Value(data={self.data}, grad={self.grad}, prev={self._prev}, op={self._op})\"\n",
    "\n",
    "\tdef __add__(self, other):\n",
    "\t\t# Implement addition here\n",
    "\t\tother = other if isinstance(other, Value) else Value(other)\n",
    "\t\tout = Value(self.data + other.data, (self, other), '+')\n",
    "\t\tdef _backward():\n",
    "\t\t\tself.grad +=out.grad\n",
    "\t\t\tother.grad += out.grad\n",
    "\t\tout._backward = _backward\n",
    "\t\treturn out\n",
    "\n",
    "\tdef __mul__(self, other):\n",
    "\t\t# Implement multiplication here\n",
    "\t\tother = other if isinstance(other, Value) else Value(other)\n",
    "\t\tout = Value(self.data * other.data, (self, other), '*')\n",
    "\t\tdef _backward():\n",
    "\t\t\tself.grad += other.data * out.grad\n",
    "\t\t\tother.grad += self.data * out.grad\n",
    "\t\tout._backward = _backward\n",
    "\t\treturn out\n",
    "\n",
    "\tdef relu(self):\n",
    "\t\tout = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "\t\tdef _backward():\n",
    "\t\t\tself.grad += (out.data > 0) * out.grad\n",
    "\t\tout._backward = _backward\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\tdef backward(self):\n",
    "\t\t# topological order all of the children in the graph\n",
    "\t\ttopo = []\n",
    "\t\tvisited = set()\n",
    "\t\tdef build_topo(v):\n",
    "\t\t\tif v not in visited:\n",
    "\t\t\t\tvisited.add(v)\n",
    "\t\t\t\tfor child in v._prev:\n",
    "\t\t\t\t\tbuild_topo(child)\n",
    "\t\t\t\ttopo.append(v)\n",
    "\t\tbuild_topo(self)\n",
    "\n",
    "\t\t# go one variable at a time and apply the chain rule to get its gradient\n",
    "\t\tself.grad = 1\n",
    "\t\tfor v in reversed(topo):\n",
    "\t\t\tv._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8a5cb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: Value(data=2.0, grad=3.0, prev=set(), op=)\n",
      "b: Value(data=3.0, grad=2.0, prev=set(), op=)\n",
      "c: Value(data=6.0, grad=1, prev={Value(data=3.0, grad=2.0, prev=set(), op=), Value(data=2.0, grad=3.0, prev=set(), op=)}, op=*)\n",
      "a: Value(data=2.0, grad=1, prev=set(), op=)\n",
      "b: Value(data=3.0, grad=1, prev=set(), op=)\n",
      "d: Value(data=5.0, grad=1, prev={Value(data=3.0, grad=1, prev=set(), op=), Value(data=2.0, grad=1, prev=set(), op=)}, op=+)\n"
     ]
    }
   ],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "c = a * b\n",
    "\n",
    "\n",
    "c.backward()\n",
    "\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b) \n",
    "print(\"c:\", c)\n",
    "\n",
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "d = a + b\n",
    "d.backward()\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b) \n",
    "print(\"d:\", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aeae355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "# DO NOT CHANGE SEED\n",
    "np.random.seed(42)\n",
    "\n",
    "# DO NOT CHANGE LAYER CLASS\n",
    "class Layer(object):\n",
    "\n",
    "\tdef set_input_shape(self, shape):\n",
    "\t\n",
    "\t\tself.input_shape = shape\n",
    "\n",
    "\tdef layer_name(self):\n",
    "\t\treturn self.__class__.__name__\n",
    "\n",
    "\tdef parameters(self):\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef forward_pass(self, X, training):\n",
    "\t\traise NotImplementedError()\n",
    "\n",
    "\tdef backward_pass(self, accum_grad):\n",
    "\t\traise NotImplementedError()\n",
    "\n",
    "\tdef output_shape(self):\n",
    "\t\traise NotImplementedError()\n",
    "\n",
    "# Your task is to implement the Dense class based on the above structure\n",
    "class Dense(Layer):\n",
    "\tdef __init__(self, n_units, input_shape=None):\n",
    "\t\tself.layer_input = None\n",
    "\t\tself.input_shape = input_shape\n",
    "\t\tself.n_units = n_units\n",
    "\t\tself.trainable = True\n",
    "\t\tself.W = None\n",
    "\t\tself.w0 = None\n",
    "\t\tself.optimizer_w = None\n",
    "\t\tself.optimizer_w0 = None\n",
    "\t\n",
    "\tdef initialize(self, optimizer):\n",
    "\t\tlimit  = 1 / np.sqrt(self.input_shape)\n",
    "\t\tself.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n",
    "\t\tself.w0 = np.zeros((1,self.n_units))\n",
    "\t\tself.optimizer_w = optimizer\n",
    "\t\tself.optimizer_w0 = optimizer\n",
    "\n",
    "\tdef forward_pass(self,x):\n",
    "\t\tself.x = x\n",
    "\t\treturn np.dot(x, self.W) + self.w0\n",
    "\n",
    "\tdef backward_pass(self, accum_grad):\n",
    "\t\tgrad_w = np.dot(self.x.T, accum_grad)\n",
    "\t\tprint(\"grad_w:\", grad_w)\n",
    "\t\tgrad_b = np.sum(accum_grad, axis=0, keepdims =True)\n",
    "\t\tprint(\"grad_b:\", grad_b)\n",
    "\t\tgrad_input = np.dot(accum_grad, self.W.T)\n",
    "\t\tprint(\"grad_input:\", grad_input)\n",
    "\t\tself.optimizer_w.update(self.W,grad_w)\n",
    "\t\tself.optimizer_w0.update(self.w0,grad_b)\n",
    "\t\treturn grad_input\n",
    "\n",
    "\tdef number_of_parameters(self):\n",
    "\t\treturn np.prod(self.W.shape) + np.prod(self.w0.shape),self.W.shape,self.w0.shape\n",
    "\t\n",
    "\tdef output_shape(self):\n",
    "\t\treturn (self.input_shape, self.n_units)\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c90a3b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Dense layer with 3 neurons and input shape (2,)\n",
    "dense_layer = Dense(n_units=3, input_shape=(2,))\n",
    "\n",
    "# Define a mock optimizer with a simple update rule\n",
    "class MockOptimizer:\n",
    "    def update(self, weights, grad):\n",
    "        return weights - 0.01 * grad\n",
    "\n",
    "optimizer = MockOptimizer()\n",
    "\n",
    "# Initialize the Dense layer with the mock optimizer\n",
    "dense_layer.initialize(optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1928edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass output: [[ 0.10162127 -0.33551992 -0.64490545]]\n",
      "grad_w: [[0.1 0.2 0.3]\n",
      " [0.2 0.4 0.6]]\n",
      "grad_b: [[0.1 0.2 0.3]]\n",
      "grad_input: [[ 0.20816524 -0.22928937]]\n",
      "Backward pass output: [[ 0.20816524 -0.22928937]]\n"
     ]
    }
   ],
   "source": [
    "# Perform a forward pass with sample input data\n",
    "X = np.array([[1, 2]])\n",
    "output = dense_layer.forward_pass(X)\n",
    "print(\"Forward pass output:\", output)\n",
    "\n",
    "# Perform a backward pass with sample gradient\n",
    "accum_grad = np.array([[0.1, 0.2, 0.3]])\n",
    "back_output = dense_layer.backward_pass(accum_grad)\n",
    "print(\"Backward pass output:\", back_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c49eea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "input_shape=(2,)\n",
    "input_shape[0]\n",
    "para, w, w0 = dense_layer.number_of_parameters()\n",
    "print(w)\n",
    "print(w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "134eae94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(2,)\n",
      "(2, 1)\n",
      "[[1]\n",
      " [2]]\n",
      "(2,)\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 2]])\n",
    "y = np.array([1, 2])\n",
    "z = X.T\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(z.shape)\n",
    "print(z)\n",
    "x = np.array([1.0, 2.0])\n",
    "print(x.shape)\n",
    "w1 = np.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "print(w1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3dd6d424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "s = 6\n",
    "a = 2\n",
    "c = s / a\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ada8342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.000088900581841e-12\n",
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def compute_cross_entropy_loss(predicted_probs: np.ndarray, true_labels: np.ndarray, epsilon = 1e-15) -> float:\n",
    "    n = len(true_labels)\n",
    "    loss = 0.0\n",
    "    for i in range(n):\n",
    "        for j in range(len(true_labels[i])):\n",
    "            if true_labels[i][j] == 1:\n",
    "                loss -= math.log(predicted_probs[i][j] + 1e-12)\n",
    "    if loss / n == -0.0:\n",
    "        return 0.0\n",
    "    return loss / n\n",
    "pred = np.array([[1, 0, 0], [0, 1, 0]])\n",
    "true = np.array([[1, 0, 0], [0, 1, 0]])\n",
    "print(compute_cross_entropy_loss(pred, true))\n",
    "print(round(compute_cross_entropy_loss(pred, true), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2de4dc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "[5.5 6.5 7.5]\n",
      "6.5\n",
      "[[4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
    "print(x.shape)\n",
    "print(np.mean(x, axis=(0,1)))\n",
    "print(np.mean(x))\n",
    "print(np.mean(x, axis=(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LSTM:\n",
    "\tdef __init__(self, input_size, hidden_size):\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\n",
    "\t\t# Initialize weights and biases\n",
    "\t\tself.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "\t\tself.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "\t\tself.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "\t\tself.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "\n",
    "\t\tself.bf = np.zeros((hidden_size, 1))\n",
    "\t\tself.bi = np.zeros((hidden_size, 1))\n",
    "\t\tself.bc = np.zeros((hidden_size, 1))\n",
    "\t\tself.bo = np.zeros((hidden_size, 1))\n",
    "\n",
    "\tdef _sigmoid(self, x):\n",
    "\t\treturn 1 / (1 + np.exp(-x))\n",
    "\t\n",
    "\tdef _tanh(self, x):\n",
    "\t\treturn np.tanh(x)\n",
    "\n",
    "\tdef forward(self, x, initial_hidden_state, initial_cell_state):\n",
    "\t\t\"\"\"\n",
    "\t\tProcesses a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n",
    "\t\t\"\"\"\n",
    "\t\tc_prev = initial_cell_state\n",
    "\t\th_prev = initial_hidden_state\n",
    "\t\toutputs = []\n",
    "\t\tseq_len = x.shape[0]    #  x (length, input_size)\n",
    "\t\tfor t in range(seq_len):\n",
    "\t\t\txt = x[t].reshape(-1, 1)  # xt (input_size, 1)\n",
    "\t\t\tcombined = np.vstack((xt, h_prev)) # combined (input_size + hidden_size, 1)\n",
    "\n",
    "\n",
    "\t\t\t# Wf * [h_prev, x_t] + bf\n",
    "\t\t\t# W is (hidden_size, input_size + hidden_size)\n",
    "\t\t\t# bf is (hidden_size, 1)\n",
    "\t\t\t# f is (hidden_size, 1)\n",
    "\t\t\tf_t = self._sigmoid(np.dot(self.Wf, combined) + self.bf)\n",
    "\t\t\t\n",
    "\t\t\ti_t = self._sigmoid(np.dot(self.Wi, combined) + self.bi)\n",
    "\n",
    "\t\t\tc_tilde = self._tanh(np.dot(self.Wc, combined) + self.bc)\n",
    "\n",
    "\t\t\to_t = self._sigmoid(np.dot(self.Wo, combined) + self.bo)\n",
    "\n",
    "\t\t\tc_prev = f_t * c_prev + i_t * c_tilde\n",
    "\t\t\th_prev = o_t * self._tanh(c_prev)\n",
    "\n",
    "\t\t\toutputs.append(h_prev.copy())\n",
    "\t\tprint(h_prev)\n",
    "\t\treturn np.array(outputs), h_prev, c_prev\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "febe0631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1]\n",
      " [0.2]]\n",
      "[[0.3]\n",
      " [0.4]]\n",
      "[[0.10978204]\n",
      " [0.33859662]]\n",
      "[[0.10978204]\n",
      " [0.33859662]]\n"
     ]
    }
   ],
   "source": [
    "input_sequence = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
    "initial_hidden_state = np.zeros((2, 1))\n",
    "initial_cell_state = np.zeros((2, 1))\n",
    "\n",
    "lstm = LSTM(input_size=2, hidden_size=2)\n",
    "lstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \n",
    "lstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \n",
    "lstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \n",
    "lstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \n",
    "lstm.bf = np.array([[0.1], [0.2]]) \n",
    "lstm.bi = np.array([[0.1], [0.2]]) \n",
    "lstm.bc = np.array([[0.1], [0.2]]) \n",
    "lstm.bo = np.array([[0.1], [0.2]])\n",
    "outputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\n",
    "\n",
    "print(final_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7610e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = np.array([[0.1, 0.2], [0.3, 0.4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4dbb881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.4]\n",
      "[[0.3]\n",
      " [0.4]]\n",
      "[[0.]\n",
      " [0.]]\n",
      "[[0.3]\n",
      " [0.4]\n",
      " [0. ]\n",
      " [0. ]]\n"
     ]
    }
   ],
   "source": [
    "print(input_sequence[1])\n",
    "xt = input_sequence[1].reshape(-1, 1) \n",
    "print(xt)\n",
    "initial_hidden_state = np.zeros((2, 1))\n",
    "print(initial_hidden_state)\n",
    "print(np.vstack((xt, initial_hidden_state)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "308c546a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c [[0.08001526]\n",
      " [0.20937158]]\n",
      "c [[0.19591405]\n",
      " [0.5543745 ]]\n",
      "Final hidden state:\n",
      " [[0.10978204]\n",
      " [0.33859662]]\n",
      "Final cell state:\n",
      " [[0.19591405]\n",
      " [0.5543745 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LSTM:\n",
    "\tdef __init__(self, input_size, hidden_size):\n",
    "\t\t\"\"\"\n",
    "\t\tInitializes the LSTM with random weights and zero biases.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tinput_size (int): Size of the input vector.\n",
    "\t\t\thidden_size (int): Size of the hidden state and cell state.\n",
    "\t\t\"\"\"\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\n",
    "\t\t# Weight matrices for forget gate, input gate, candidate cell state, and output gate\n",
    "\t\tself.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "\t\tself.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "\t\tself.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "\t\tself.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "\n",
    "\t\t# Bias vectors for forget gate, input gate, candidate cell state, and output gate\n",
    "\t\tself.bf = np.zeros((hidden_size, 1))\n",
    "\t\tself.bi = np.zeros((hidden_size, 1))\n",
    "\t\tself.bc = np.zeros((hidden_size, 1))\n",
    "\t\tself.bo = np.zeros((hidden_size, 1))\n",
    "\n",
    "\tdef forward(self, x, initial_hidden_state, initial_cell_state):\n",
    "\t\t\"\"\"\n",
    "\t\tProcesses a sequence of inputs and returns the hidden states at each time step,\n",
    "\t\tas well as the final hidden state and cell state.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tx (np.ndarray): Input sequence of shape (T, input_size), where T is the sequence length.\n",
    "\t\t\tinitial_hidden_state (np.ndarray): Initial hidden state of shape (hidden_size, 1).\n",
    "\t\t\tinitial_cell_state (np.ndarray): Initial cell state of shape (hidden_size, 1).\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\ttuple: (hidden_states, final_hidden_state, final_cell_state)\n",
    "\t\t\t\t- hidden_states: List of hidden states at each time step.\n",
    "\t\t\t\t- final_hidden_state: Final hidden state after processing the sequence.\n",
    "\t\t\t\t- final_cell_state: Final cell state after processing the sequence.\n",
    "\t\t\"\"\"\n",
    "\t\tT, _ = x.shape\n",
    "\t\th = initial_hidden_state\n",
    "\t\tc = initial_cell_state\n",
    "\t\thidden_states = []\n",
    "\n",
    "\t\tfor t in range(T):\n",
    "\t\t\txt = x[t].reshape(-1, 1)  # Current input at time step t\n",
    "\n",
    "\t\t\t# Concatenate input and previous hidden state\n",
    "\t\t\tcombined = np.vstack((xt, h))\n",
    "\n",
    "\t\t\t# Forget gate\n",
    "\t\t\tft = self.sigmoid(np.dot(self.Wf, combined) + self.bf)\n",
    "\n",
    "\t\t\t# Input gate\n",
    "\t\t\tit = self.sigmoid(np.dot(self.Wi, combined) + self.bi)\n",
    "\n",
    "\t\t\t# Candidate cell state\n",
    "\t\t\tct_tilde = np.tanh(np.dot(self.Wc, combined) + self.bc)\n",
    "\n",
    "\t\t\t# Update cell state\n",
    "\t\t\tc = ft * c + it * ct_tilde\n",
    "\t\t\tprint(\"c\",c)\n",
    "\t\t\t# Output gate\n",
    "\t\t\tot = self.sigmoid(np.dot(self.Wo, combined) + self.bo)\n",
    "\n",
    "\t\t\t# Update hidden state\n",
    "\t\t\th = ot * np.tanh(c)\n",
    "\n",
    "\t\t\t# Store the hidden state\n",
    "\t\t\thidden_states.append(h)\n",
    "\n",
    "\t\thidden_states = np.hstack(hidden_states)  # Convert list to array\n",
    "\t\treturn hidden_states, h, c\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef sigmoid(x):\n",
    "\t\t\"\"\"\n",
    "\t\tSigmoid activation function.\n",
    "\t\t\"\"\"\n",
    "\t\treturn 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\tinput_sequence = np.array([[0.1, 0.2], [0.3, 0.4]])  # Shape: (T=2, input_size=2)\n",
    "\tinitial_hidden_state = np.zeros((2, 1))  # Shape: (hidden_size=2, 1)\n",
    "\tinitial_cell_state = np.zeros((2, 1))  # Shape: (hidden_size=2, 1)\n",
    "\n",
    "\tlstm = LSTM(input_size=2, hidden_size=2)\n",
    "\n",
    "\t# Set weights and biases for reproducibility (optional)\n",
    "\tlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\n",
    "\tlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\n",
    "\tlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\n",
    "\tlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\n",
    "\tlstm.bf = np.array([[0.1], [0.2]])\n",
    "\tlstm.bi = np.array([[0.1], [0.2]])\n",
    "\tlstm.bc = np.array([[0.1], [0.2]])\n",
    "\tlstm.bo = np.array([[0.1], [0.2]])\n",
    "\n",
    "\toutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\n",
    "\tprint(\"Final hidden state:\\n\", final_h)\n",
    "\tprint(\"Final cell state:\\n\", final_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b62fa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10978204]\n",
      " [0.33859662]]\n"
     ]
    }
   ],
   "source": [
    "input_sequence = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
    "initial_hidden_state = np.zeros((2, 1))\n",
    "initial_cell_state = np.zeros((2, 1))\n",
    "\n",
    "lstm = LSTM(input_size=2, hidden_size=2)\n",
    "lstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \n",
    "lstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \n",
    "lstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \n",
    "lstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \n",
    "lstm.bf = np.array([[0.1], [0.2]]) \n",
    "lstm.bi = np.array([[0.1], [0.2]]) \n",
    "lstm.bc = np.array([[0.1], [0.2]]) \n",
    "lstm.bo = np.array([[0.1], [0.2]])\n",
    "outputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\n",
    "\n",
    "print(final_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d9ec1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pos_encoding_elem(pos, d_model):\n",
    "    pos_encode = np.zeros(d_model)\n",
    "    for i in range(0,d_model,2):\n",
    "        angle = pos / np.power(10000, (2*i)/d_model)\n",
    "        pos_encode[i] = np.sin(angle)\n",
    "        if i + 1< d_model:\n",
    "            pos_encode[i + 1] = np.cos(angle)\n",
    "    return pos_encode\n",
    "\n",
    "# output (position, d_model)\n",
    "def pos_encoding(position: int, d_model: int):\n",
    "\t# Your code here\n",
    "    if position == 0 or d_model <= 0:\n",
    "        return -1\n",
    "    else:\n",
    "        pos_encoding = np.zeros((position,d_model))\n",
    "        for pos in range(position):\n",
    "            pos_encoding[pos] = pos_encoding_elem(pos, d_model)\n",
    "        return np.round(pos_encoding,4)\n",
    "\n",
    "def _get_position_encoding_matrix(max_seq_len: int, d_model: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates position encoding matrix using vectorized operations.\n",
    "    \n",
    "    Args:\n",
    "        max_seq_len (int): Maximum sequence length.\n",
    "        d_model (int): Dimension of the model/embedding.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Position encoding matrix of shape (max_seq_len, d_model).\n",
    "    \"\"\"\n",
    "    # Create position and dimension indices\n",
    "    positions = np.arange(max_seq_len)[:, np.newaxis]     # Shape: (max_seq_len, 1)\n",
    "    dims = np.arange(0, d_model, 2)[np.newaxis, :]       # Shape: (1, d_model/2)\n",
    "    \n",
    "    # Calculate angles using broadcasting\n",
    "    angles = positions / np.power(10000, (2 * dims) / d_model)  # Shape: (max_seq_len, d_model/2)\n",
    "    \n",
    "    # Apply sin and cos functions\n",
    "    pos_encoding = np.empty((max_seq_len, d_model))\n",
    "    pos_encoding[:, 0::2] = np.sin(angles)\n",
    "    pos_encoding[:, 1::2] = np.cos(angles)\n",
    "    \n",
    "    return pos_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "926e3446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000e+00 1.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  1.000e+00]\n",
      " [8.415e-01 5.403e-01 1.000e-02 1.000e+00 1.000e-04 1.000e+00 0.000e+00\n",
      "  1.000e+00]]\n",
      "[[0.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00]\n",
      " [8.41470985e-01 5.40302306e-01 9.99983333e-03 9.99950000e-01\n",
      "  9.99999998e-05 9.99999995e-01 1.00000000e-06 1.00000000e+00]]\n",
      "[[ 0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      " [ 8.41470985e-01  5.40302306e-01  9.98334166e-02  9.95004165e-01\n",
      "   9.99983333e-03  9.99950000e-01  9.99999833e-04  9.99999500e-01\n",
      "   9.99999998e-05  9.99999995e-01  1.00000000e-05  1.00000000e+00\n",
      "   1.00000000e-06  1.00000000e+00  1.00000000e-07  1.00000000e+00]\n",
      " [ 9.09297427e-01 -4.16146837e-01  1.98669331e-01  9.80066578e-01\n",
      "   1.99986667e-02  9.99800007e-01  1.99999867e-03  9.99998000e-01\n",
      "   1.99999999e-04  9.99999980e-01  2.00000000e-05  1.00000000e+00\n",
      "   2.00000000e-06  1.00000000e+00  2.00000000e-07  1.00000000e+00]\n",
      " [ 1.41120008e-01 -9.89992497e-01  2.95520207e-01  9.55336489e-01\n",
      "   2.99955002e-02  9.99550034e-01  2.99999550e-03  9.99995500e-01\n",
      "   2.99999995e-04  9.99999955e-01  3.00000000e-05  1.00000000e+00\n",
      "   3.00000000e-06  1.00000000e+00  3.00000000e-07  1.00000000e+00]\n",
      " [-7.56802495e-01 -6.53643621e-01  3.89418342e-01  9.21060994e-01\n",
      "   3.99893342e-02  9.99200107e-01  3.99998933e-03  9.99992000e-01\n",
      "   3.99999989e-04  9.99999920e-01  4.00000000e-05  9.99999999e-01\n",
      "   4.00000000e-06  1.00000000e+00  4.00000000e-07  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(pos_encoding(2, 8))\n",
    "print(_get_position_encoding_matrix(2, 8))\n",
    "print(_get_position_encoding_matrix(5, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db1afba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from turtle import forward\n",
    "import numpy as np\n",
    "class SimpleRNN:\n",
    "\tdef __init__(self, input_size, hidden_size, output_size):\n",
    "\t\t\"\"\"\n",
    "\t\tInitializes the RNN with random weights and zero biases.\n",
    "\t\t\"\"\"\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.W_xh = np.random.randn(hidden_size, input_size)*0.01\n",
    "\t\tself.W_hh = np.random.randn(hidden_size, hidden_size)*0.01\n",
    "\t\tself.W_hy = np.random.randn(output_size, hidden_size)*0.01\n",
    "\t\tself.b_h = np.zeros((hidden_size, 1))\n",
    "\t\tself.b_y = np.zeros((output_size, 1))\n",
    "\n",
    "\t\tself.input_size =  input_size\n",
    "\t\tself.h_out = []\n",
    "\t\tself.pred_out = []\n",
    "\t\tself.loss_t = []\n",
    "\t\tself.loss = 0.0\n",
    "\tdef forward(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tForward pass through the RNN for a given sequence of inputs.\n",
    "\t\t\"\"\"\n",
    "\t\th = np.zeros((self.hidden_size, 1))\n",
    "\t\tself.h_out.append(h)\n",
    "\t\tfor i in range(x.shape[0]):\n",
    "\t\t\txi = x[i].reshape(-1,1)\n",
    "\t\t\th = np.tanh(np.dot(self.W_xh, xi) + np.dot(self.W_hh, h) + self.b_h)\n",
    "\t\t\tself.h_out.append(h)\n",
    "\t\t\toutput =  np.dot(self.W_hy , h) + self.b_y\n",
    "\t\t\tself.pred_out.append(output)\n",
    "\t\treturn self.pred_out\n",
    "\t\n",
    "\n",
    "\tdef backward(self, x, y, learning_rate):\n",
    "\t\t\"\"\"\n",
    "\t\tBackpropagation through time to adjust weights based on error gradient.\n",
    "\t\t\"\"\"\n",
    "\t\tdWx = np.zeros_like(self.W_xh)\n",
    "\t\tdWh = np.zeros_like(self.W_hh)\n",
    "\t\tdb = np.zeros_like(self.W_hy)\n",
    "\t\tdV = np.zeros_like(self.b_h)\n",
    "\t\tdc = np.zeros_like(self.b_y)\n",
    "\n",
    "\t\tdht_next = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "\t\t\t   \n",
    "\t\t# Loop backward through time steps\n",
    "\t\tfor t in reversed(range(x.shape[0])):\n",
    "\t\t\tself.forward( x)\n",
    "\t\t\tyt_true = y[t].reshape(-1,1)\n",
    "\t\t\terror_t = self.pred_out[t] - yt_true\n",
    "\t\t\tself.loss += 0.5 * np.sum(error_t ** 2)\n",
    "\t\t\tself.loss_t.append(error_t)\n",
    "\n",
    "\t\t\t# Gradient from the output layer at time t\n",
    "\t\t\tdYt = error_t   # shape: (output_size, 1)\n",
    "\t\t\t# Gradients for output layer parameters\n",
    "\t\t\tprint(dYt)\n",
    "\t\t\tprint(self.h_out[t])\n",
    "\t\t\tdV += np.dot(dYt, self.h_out[t].T)\n",
    "\t\t\tdc += dYt\n",
    "\n",
    "\t\t\t# Backpropagate into the hidden state (from output and future time steps)\n",
    "\t\t\tdht = np.dot(self.b_h.T, dYt) + dht_next  # shape: (hidden_size, 1)\n",
    "\t\t\t\n",
    "\t\t\t# Derivative through tanh activation: tanh derivative = (1 - h^2)\n",
    "\t\t\tdtanh = (1 - self.h_out[t] ** 2) * dht  # shape: (hidden_size, 1)\n",
    "\t\t\t\n",
    "\t\t\t# Current input at time step t\n",
    "\t\t\txt = x[t].reshape(-1, 1)  # shape: (input_size, 1)\n",
    "\t\t\t\n",
    "\t\t\t# Gradients with respect to Wx, Wh, and bias\n",
    "\t\t\tdWx += np.dot(dtanh, xt.T)\n",
    "\t\t\tdWh += np.dot(dtanh, self.h[t-1].T)\n",
    "\t\t\tdb  += dtanh\n",
    "\t\t\t\n",
    "\t\t\t# Propagate the gradient to previous hidden state\n",
    "\t\t\tdht_next = np.dot(self.Wh.T, dtanh)\n",
    "\t\t\n",
    "\t\t# Update parameters using the provided learning rate\n",
    "\t\tself.W_hx -= learning_rate * dWx\n",
    "\t\tself.W_hh -= learning_rate * dWh\n",
    "\t\tself.W_hy  -= learning_rate * db\n",
    "\t\tself.b_h  -= learning_rate * dV\n",
    "\t\tself.b_y  -= learning_rate * dc\n",
    "\t\t\n",
    "\t\treturn dWx, dWh, db, dV, dc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79db3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(values):\n",
    "\tvalues = values - np.max(values, -1, keepdims = True)\n",
    "\texp_x = np.exp(values)\n",
    "\treturn exp_x / np.sum(exp_x, -1, keepdims = True)\n",
    "\n",
    "# crystal_values (n, dimension)\n",
    "def pattern_weaver(n, crystal_values, dimension):\n",
    "\tcrystal_values = np.array(crystal_values)\n",
    "\tprint(crystal_values.shape)\n",
    "\t# Compute similarity matrix using dot product:\n",
    "\tlogits = np.dot(crystal_values, crystal_values.T)\n",
    "\tprint(logits)\n",
    "\t# Compute attention scores using softmax along each row:\n",
    "\tattn = softmax(logits)\n",
    "\tprint(attn)\n",
    "\t# For each crystal, compute the weighted sum of all crystals:\n",
    "\tweighted = np.dot(attn, crystal_values)\n",
    "\treturn weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "60139ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "151\n",
      "1.0\n",
      "[4. 2. 7. 1. 9.]\n"
     ]
    }
   ],
   "source": [
    "print(pattern_weaver(5, [4, 2, 7, 1, 9], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa02663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_qkv(X, W_q, W_k, W_v):\n",
    "\tQ = np.matmul(X, W_q)\n",
    "\tK = np.matmul(X, W_k)\n",
    "\tV = np.matmul(X, W_v)\n",
    "\treturn Q,K,V\n",
    "\n",
    "def split_heads(x, n_heads):\n",
    "\t\"\"\"Split the last dimension into (n_heads, head_dim).\"\"\"\n",
    "\tseq_len, d_model = x.shape\n",
    "\thead_dim = d_model // n_heads\n",
    "\treturn x.reshape(seq_len, n_heads, head_dim).transpose(1, 0, 2)\n",
    "\n",
    "def combine_heads(x):\n",
    "\t\"\"\"Combine the head dimensions back to the original dimension.\"\"\"\n",
    "\tn_heads, seq_len, head_dim = x.shape\n",
    "\treturn x.transpose(1, 0, 2).reshape(seq_len, n_heads * head_dim)\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "\t\"\"\"Calculate the attention weights and output.\"\"\"\n",
    "\td_k = q.shape[-1]\n",
    "\tscores = np.matmul(q, k.transpose(0, 2, 1)) / np.sqrt(d_k)\n",
    "\t\n",
    "\tif mask is not None:\n",
    "\t\tscores = scores + mask\n",
    "\t\n",
    "\tattention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
    "\tattention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n",
    "\t\n",
    "\toutput = np.matmul(attention_weights, v)\n",
    "\treturn output\n",
    "\n",
    "def multi_head_attention(q, k, v, n_heads=2, mask=None):\n",
    "\t\"\"\"Multi-head attention mechanism without batch dimension.\"\"\"\n",
    "\tassert q.shape[-1] % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\t\n",
    "\t# Split into multiple heads\n",
    "\tq_split = split_heads(q, n_heads)  # [n_heads, seq_len, head_dim]\n",
    "\tk_split = split_heads(k, n_heads)\n",
    "\tv_split = split_heads(v, n_heads)\n",
    "\t\n",
    "\t# Compute attention for each head\n",
    "\tattention_output = scaled_dot_product_attention(q_split, k_split, v_split, mask)\n",
    "\t\n",
    "\t# Combine heads back together\n",
    "\tcombined = combine_heads(attention_output)  # [seq_len, d_model]\n",
    "\t\n",
    "\treturn combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5f78f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[432 381 409 366]\n",
      "  [336 429 420 288]]\n",
      "\n",
      " [[300 157 254 300]\n",
      "  [178 338 180 183]]\n",
      "\n",
      " [[392 286 435 376]\n",
      "  [267 481 377 298]]\n",
      "\n",
      " [[329 214 370 257]\n",
      "  [199 405 315 307]]\n",
      "\n",
      " [[412 263 315 294]\n",
      "  [213 389 298 324]]\n",
      "\n",
      " [[330 340 418 345]\n",
      "  [320 443 370 279]]]\n",
      "[[[570 504 491 531]\n",
      "  [304 449 307 408]]\n",
      "\n",
      " [[357 344 315 311]\n",
      "  [185 369 285 292]]\n",
      "\n",
      " [[622 434 417 529]\n",
      "  [265 539 363 428]]\n",
      "\n",
      " [[427 284 284 307]\n",
      "  [176 376 360 349]]\n",
      "\n",
      " [[473 411 400 491]\n",
      "  [235 371 358 342]]\n",
      "\n",
      " [[701 418 350 553]\n",
      "  [294 498 286 382]]]\n",
      "[[[547 490 399 495]\n",
      "  [485 439 645 393]]\n",
      "\n",
      " [[203 293 199 399]\n",
      "  [220 163 279 174]]\n",
      "\n",
      " [[471 472 429 538]\n",
      "  [377 450 531 362]]\n",
      "\n",
      " [[398 388 244 338]\n",
      "  [224 385 413 295]]\n",
      "\n",
      " [[541 331 366 492]\n",
      "  [475 367 423 422]]\n",
      "\n",
      " [[500 463 514 447]\n",
      "  [403 500 556 496]]]\n",
      "[[547. 490. 399. 495. 377. 450. 531. 362.]\n",
      " [547. 490. 399. 495. 377. 450. 531. 362.]\n",
      " [547. 490. 399. 495. 377. 450. 531. 362.]\n",
      " [547. 490. 399. 495. 377. 450. 531. 362.]\n",
      " [547. 490. 399. 495. 377. 450. 531. 362.]\n",
      " [547. 490. 399. 495. 377. 450. 531. 362.]]\n"
     ]
    }
   ],
   "source": [
    "m, n = 6, 8 \n",
    "n_heads = 2 \n",
    "np.random.seed(42) \n",
    "X = np.arange(m*n).reshape(m,n) \n",
    "X = np.random.permutation(X.flatten()).reshape(m, n) \n",
    "W_q = np.random.randint(0,4,size=(n,n)) \n",
    "W_k = np.random.randint(0,5,size=(n,n)) \n",
    "W_v = np.random.randint(0,6,size=(n,n)) \n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v) \n",
    "test = Q\n",
    "# test multi-head attention\n",
    "actual_output = multi_head_attention(Q, K, V, n_heads) \n",
    "print(actual_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a248c265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "[[[1. 0.]\n",
      "  [1. 0.]]\n",
      "\n",
      " [[0. 1.]\n",
      "  [1. 0.]]\n",
      "\n",
      " [[0. 1.]\n",
      "  [1. 0.]]\n",
      "\n",
      " [[0. 1.]\n",
      "  [1. 0.]]]\n",
      "hello-----------\n",
      "[[[1. 0.]\n",
      "  [0. 1.]\n",
      "  [0. 1.]\n",
      "  [0. 1.]]\n",
      "\n",
      " [[1. 0.]\n",
      "  [1. 0.]\n",
      "  [1. 0.]\n",
      "  [1. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "test = np.array([[1., 0.,1., 0.], [0., 1.,1., 0.],[0., 1.,1., 0.],[0., 1.,1., 0.]])\n",
    "print(test.shape)\n",
    "print(test.reshape(4, 2, 2))\n",
    "print(\"hello-----------\")\n",
    "print(test.reshape(4, 2, 2).transpose(1, 0, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c50bc491",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "160b98a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d8ef9502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution(A, B):\n",
    "    max_possible = min(A, B)\n",
    "    left = 1\n",
    "    right = max_possible\n",
    "    best = 0\n",
    "    \n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        total_sticks = (A // mid) + (B // mid)\n",
    "        if total_sticks >= 4:\n",
    "            best = mid\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "61067a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(solution(10,21))\n",
    "print(solution(13,11))\n",
    "print(solution(2,1))\n",
    "print(solution(1,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6bed2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution(A, B):\n",
    "    max_possible = min(A, B)\n",
    "    left = 1\n",
    "    right = max_possible\n",
    "    best = 0\n",
    "    \n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        total_sticks = (A // mid) + (B // mid)\n",
    "        if total_sticks >= 4:\n",
    "            best = mid\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e4767acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(solution(1,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "65b09837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution(letters):\n",
    "    # 如果 letters 不是字符串，则尝试将其转换为字符串\n",
    "    #if not isinstance(letters, str):\n",
    "    #    letters = ''.join(letters)\n",
    "    \n",
    "    count = 0\n",
    "    for c in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        lower = c\n",
    "        upper = c.upper()\n",
    "        if lower in letters and upper in letters:\n",
    "            last_lower = letters.rfind(lower)\n",
    "            first_upper = letters.find(upper)\n",
    "            if last_lower < first_upper:\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ed26c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution2(letters):\n",
    "    \"\"\"\n",
    "    Counts the number of different letters that appear in both lowercase and uppercase,\n",
    "    where all lowercase occurrences appear before any uppercase occurrence.\n",
    "\n",
    "    Args:\n",
    "        letters (str): The input string of English letters.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of different letters fulfilling the conditions.\n",
    "    \"\"\"\n",
    "    seen_lower = set()  # Set to track lowercase letters seen\n",
    "    invalid_letters = set()  # Set to track letters that violate the condition\n",
    "    valid_letters = set()  # Set to track valid letters\n",
    "\n",
    "    for char in letters:\n",
    "        if char.islower():\n",
    "            # If it's a lowercase letter, add it to the seen_lower set\n",
    "            if char not in invalid_letters:\n",
    "                seen_lower.add(char)\n",
    "        elif char.isupper():\n",
    "            # If it's an uppercase letter\n",
    "            lower_char = char.lower()\n",
    "            if lower_char not in seen_lower:\n",
    "                # If the corresponding lowercase letter has not been seen, it's invalid\n",
    "                invalid_letters.add(lower_char)\n",
    "                if lower_char in valid_letters:\n",
    "                    valid_letters.remove(lower_char)  # Remove from valid if previously added\n",
    "            else:\n",
    "                # If the lowercase letter was seen before, it's valid\n",
    "                if lower_char not in invalid_letters:\n",
    "                    valid_letters.add(lower_char)\n",
    "\n",
    "    # Return the count of valid letters\n",
    "    return len(valid_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "963b124e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "6\n",
      "6\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "letters = \"aaAbcCABBc\"\n",
    "print(solution(letters))\n",
    "print(solution2(letters))\n",
    "letters = \"xyzXYZabcABC\"\n",
    "print(solution(letters))\n",
    "print(solution2(letters))\n",
    "letters = \"ABCabcAefG\"\n",
    "print(solution(letters))\n",
    "print(solution2(letters))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
