{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40a3f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ead478a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [[1 2 3 4]\n",
      " [5 6 7 8]]\n",
      "b [[1 2]\n",
      " [3 4]\n",
      " [5 6]\n",
      " [7 8]]\n"
     ]
    }
   ],
   "source": [
    "a = [[1,2,3,4],[5,6,7,8]]\n",
    "a = np.array(a)\n",
    "new_shape = (4,2)\n",
    "\n",
    "b = np.reshape(a, new_shape)\n",
    "\n",
    "print(\"a\" , a)\n",
    "print(\"b\" , b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b0cfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape(4,2):\n",
      " [[1 2]\n",
      " [3 4]\n",
      " [5 6]\n",
      " [7 8]]\n",
      "transpose:\n",
      " [[1 5]\n",
      " [2 6]\n",
      " [3 7]\n",
      " [4 8]]\n"
     ]
    }
   ],
   "source": [
    "# reshape vs transpose\n",
    "# reshape 只改变形状，不改变元素顺序；transpose 会交换轴：\n",
    "print(\"reshape(4,2):\\n\", a.reshape(4, 2))\n",
    "print(\"transpose:\\n\", a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "874308de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 1.5]\n",
      " [1.5 3.5]]\n"
     ]
    }
   ],
   "source": [
    "def is_invertible(a):\n",
    "    if len(a) != len(a[0]):\n",
    "        return false\n",
    "    return np.linalg.det(a) != 0\n",
    "\n",
    "def transform_matrix(A, T, S):\n",
    "    if not is_invertible(T) or not is_invertible(S) :\n",
    "        print(\"T or S is not invertible\")\n",
    "        return -1\n",
    "    else:\n",
    "        transformed_matrix = (np.linalg.inv(T) @ A) @ S\n",
    "    return transformed_matrix\n",
    "\n",
    "print(transform_matrix([[1, 2], [3, 4]], [[2, 0], [0, 2]], [[1, 1], [0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e8fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]]\n",
    "labels = [0, 1, 0]\n",
    "weights = [0.7, -0.4]\n",
    "bias = -0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "486670a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46257015 0.41338242 0.66818777]\n",
      "0.3349\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "prob =sigmoid(np.dot(features,weights) + bias)\n",
    "print(prob)\n",
    "mse = np.round(np.mean((labels - prob) **2), 4)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "593db0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47c0e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\n",
    "labels = [1, 0, 0]\n",
    "initial_weights = [0.1, -0.2]\n",
    "initial_bias = 0.0\n",
    "learning_rate = 0.1\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f21fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.38296168  0.33333333  0.38296168]\n",
      "[0.24445831 0.25       0.24445831]\n",
      "[-0.09361817  0.08333333  0.09361817]\n",
      "grad_w [-0.02056966 -0.29113933]\n",
      "grad_b 0.08333333333333334\n",
      "[-0.37446407  0.33748221  0.37172584]\n",
      "[0.24619359 0.24996127 0.24668354]\n",
      "[-0.09219065  0.08435748  0.09169864]\n",
      "grad_w [-0.01517434 -0.28342112]\n",
      "grad_b 0.08386547041703868\n",
      "weights [ 0.1035744  -0.14254396]\n",
      "bias -0.016719880375037202\n",
      "mse_loss [0.3033228034139421, 0.2942232621822798]\n"
     ]
    }
   ],
   "source": [
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "weights = np.array(initial_weights)\n",
    "bias = initial_bias\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "n = len(labels)\n",
    "mse_loss = []\n",
    "for epoch in range(epochs):\n",
    "    # foward\n",
    "    z = np.dot(features,weights) + bias\n",
    "    predict = sigmoid(z)\n",
    "    # mse loss\n",
    "    loss = np.mean((np.array(labels) - predict) ** 2)\n",
    "    mse_loss.append(loss)\n",
    "    #gradient descent\n",
    "    dloss_dpred = 2 * (predict - labels) / n   # n\n",
    "    dpred_dz = sigmoid_derivative(predict)     # n\n",
    "    dz_dw = features                           # n * m\n",
    "    dz_db = 1\n",
    "\n",
    "    print(dloss_dpred)\n",
    "    print(dpred_dz)\n",
    "    grad_z = dloss_dpred * dpred_dz  # n * n =  n\n",
    "    print(grad_z)\n",
    "    grad_w = np.dot(grad_z, dz_dw)  # n * m\n",
    "    print(\"grad_w\",grad_w)\n",
    "    grad_b = np.sum(grad_z) * dz_db \n",
    "    print(\"grad_b\", grad_b)\n",
    "    # update weights and bias\n",
    "    weights -= learning_rate*grad_w\n",
    "    bias -= learning_rate*grad_b\n",
    "\n",
    "print(\"weights\", weights)\n",
    "print(\"bias\", bias)\n",
    "print(\"mse_loss\", mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "012c9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "\tdef __init__(self, data, _children=(), _op=''):\n",
    "\t\tself.data = data\n",
    "\t\tself.grad = 0\n",
    "\t\tself._backward = lambda: None\n",
    "\t\tself._prev = set(_children)\n",
    "\t\tself._op = _op\n",
    "\tdef __repr__(self):\n",
    "\t\treturn f\"Value(data={self.data}, grad={self.grad}, prev={self._prev}, op={self._op})\"\n",
    "\n",
    "\tdef __add__(self, other):\n",
    "\t\t# Implement addition here\n",
    "\t\tother = other if isinstance(other, Value) else Value(other)\n",
    "\t\tout = Value(self.data + other.data, (self, other), '+')\n",
    "\t\tdef _backward():\n",
    "\t\t\tself.grad +=out.grad\n",
    "\t\t\tother.grad += out.grad\n",
    "\t\tout._backward = _backward\n",
    "\t\treturn out\n",
    "\n",
    "\tdef __mul__(self, other):\n",
    "\t\t# Implement multiplication here\n",
    "\t\tother = other if isinstance(other, Value) else Value(other)\n",
    "\t\tout = Value(self.data * other.data, (self, other), '*')\n",
    "\t\tdef _backward():\n",
    "\t\t\tself.grad += other.data * out.grad\n",
    "\t\t\tother.grad += self.data * out.grad\n",
    "\t\tout._backward = _backward\n",
    "\t\treturn out\n",
    "\n",
    "\tdef relu(self):\n",
    "\t\tout = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "\t\tdef _backward():\n",
    "\t\t\tself.grad += (out.data > 0) * out.grad\n",
    "\t\tout._backward = _backward\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\tdef backward(self):\n",
    "\t\t# topological order all of the children in the graph\n",
    "\t\ttopo = []\n",
    "\t\tvisited = set()\n",
    "\t\tdef build_topo(v):\n",
    "\t\t\tif v not in visited:\n",
    "\t\t\t\tvisited.add(v)\n",
    "\t\t\t\tfor child in v._prev:\n",
    "\t\t\t\t\tbuild_topo(child)\n",
    "\t\t\t\ttopo.append(v)\n",
    "\t\tbuild_topo(self)\n",
    "\n",
    "\t\t# go one variable at a time and apply the chain rule to get its gradient\n",
    "\t\tself.grad = 1\n",
    "\t\tfor v in reversed(topo):\n",
    "\t\t\tv._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8a5cb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: Value(data=2.0, grad=3.0, prev=set(), op=)\n",
      "b: Value(data=3.0, grad=2.0, prev=set(), op=)\n",
      "c: Value(data=6.0, grad=1, prev={Value(data=3.0, grad=2.0, prev=set(), op=), Value(data=2.0, grad=3.0, prev=set(), op=)}, op=*)\n",
      "a: Value(data=2.0, grad=1, prev=set(), op=)\n",
      "b: Value(data=3.0, grad=1, prev=set(), op=)\n",
      "d: Value(data=5.0, grad=1, prev={Value(data=3.0, grad=1, prev=set(), op=), Value(data=2.0, grad=1, prev=set(), op=)}, op=+)\n"
     ]
    }
   ],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "c = a * b\n",
    "\n",
    "\n",
    "c.backward()\n",
    "\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b) \n",
    "print(\"c:\", c)\n",
    "\n",
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "d = a + b\n",
    "d.backward()\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b) \n",
    "print(\"d:\", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aeae355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "# DO NOT CHANGE SEED\n",
    "np.random.seed(42)\n",
    "\n",
    "# DO NOT CHANGE LAYER CLASS\n",
    "class Layer(object):\n",
    "\n",
    "\tdef set_input_shape(self, shape):\n",
    "\t\n",
    "\t\tself.input_shape = shape\n",
    "\n",
    "\tdef layer_name(self):\n",
    "\t\treturn self.__class__.__name__\n",
    "\n",
    "\tdef parameters(self):\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef forward_pass(self, X, training):\n",
    "\t\traise NotImplementedError()\n",
    "\n",
    "\tdef backward_pass(self, accum_grad):\n",
    "\t\traise NotImplementedError()\n",
    "\n",
    "\tdef output_shape(self):\n",
    "\t\traise NotImplementedError()\n",
    "\n",
    "# Your task is to implement the Dense class based on the above structure\n",
    "class Dense(Layer):\n",
    "\tdef __init__(self, n_units, input_shape=None):\n",
    "\t\tself.layer_input = None\n",
    "\t\tself.input_shape = input_shape\n",
    "\t\tself.n_units = n_units\n",
    "\t\tself.trainable = True\n",
    "\t\tself.W = None\n",
    "\t\tself.w0 = None\n",
    "\t\tself.optimizer_w = None\n",
    "\t\tself.optimizer_w0 = None\n",
    "\t\n",
    "\tdef initialize(self, optimizer):\n",
    "\t\tlimit  = 1 / np.sqrt(self.input_shape)\n",
    "\t\tself.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n",
    "\t\tself.w0 = np.zeros((1,self.n_units))\n",
    "\t\tself.optimizer_w = optimizer\n",
    "\t\tself.optimizer_w0 = optimizer\n",
    "\n",
    "\tdef forward_pass(self,x):\n",
    "\t\tself.x = x\n",
    "\t\treturn np.dot(x, self.W) + self.w0\n",
    "\n",
    "\tdef backward_pass(self, accum_grad):\n",
    "\t\tgrad_w = np.dot(self.x.T, accum_grad)\n",
    "\t\tprint(\"grad_w:\", grad_w)\n",
    "\t\tgrad_b = np.sum(accum_grad, axis=0, keepdims =True)\n",
    "\t\tprint(\"grad_b:\", grad_b)\n",
    "\t\tgrad_input = np.dot(accum_grad, self.W.T)\n",
    "\t\tprint(\"grad_input:\", grad_input)\n",
    "\t\tself.optimizer_w.update(self.W,grad_w)\n",
    "\t\tself.optimizer_w0.update(self.w0,grad_b)\n",
    "\t\treturn grad_input\n",
    "\n",
    "\tdef number_of_parameters(self):\n",
    "\t\treturn np.prod(self.W.shape) + np.prod(self.w0.shape),self.W.shape,self.w0.shape\n",
    "\t\n",
    "\tdef output_shape(self):\n",
    "\t\treturn (self.input_shape, self.n_units)\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c90a3b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Dense layer with 3 neurons and input shape (2,)\n",
    "dense_layer = Dense(n_units=3, input_shape=(2,))\n",
    "\n",
    "# Define a mock optimizer with a simple update rule\n",
    "class MockOptimizer:\n",
    "    def update(self, weights, grad):\n",
    "        return weights - 0.01 * grad\n",
    "\n",
    "optimizer = MockOptimizer()\n",
    "\n",
    "# Initialize the Dense layer with the mock optimizer\n",
    "dense_layer.initialize(optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1928edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass output: [[ 0.10162127 -0.33551992 -0.64490545]]\n",
      "grad_w: [[0.1 0.2 0.3]\n",
      " [0.2 0.4 0.6]]\n",
      "grad_b: [[0.1 0.2 0.3]]\n",
      "grad_input: [[ 0.20816524 -0.22928937]]\n",
      "Backward pass output: [[ 0.20816524 -0.22928937]]\n"
     ]
    }
   ],
   "source": [
    "# Perform a forward pass with sample input data\n",
    "X = np.array([[1, 2]])\n",
    "output = dense_layer.forward_pass(X)\n",
    "print(\"Forward pass output:\", output)\n",
    "\n",
    "# Perform a backward pass with sample gradient\n",
    "accum_grad = np.array([[0.1, 0.2, 0.3]])\n",
    "back_output = dense_layer.backward_pass(accum_grad)\n",
    "print(\"Backward pass output:\", back_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c49eea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "input_shape=(2,)\n",
    "input_shape[0]\n",
    "para, w, w0 = dense_layer.number_of_parameters()\n",
    "print(w)\n",
    "print(w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "134eae94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(2,)\n",
      "(2, 1)\n",
      "[[1]\n",
      " [2]]\n",
      "(2,)\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 2]])\n",
    "y = np.array([1, 2])\n",
    "z = X.T\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(z.shape)\n",
    "print(z)\n",
    "x = np.array([1.0, 2.0])\n",
    "print(x.shape)\n",
    "w1 = np.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "print(w1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3dd6d424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "s = 6\n",
    "a = 2\n",
    "c = s / a\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ada8342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.000088900581841e-12\n",
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def compute_cross_entropy_loss(predicted_probs: np.ndarray, true_labels: np.ndarray, epsilon = 1e-15) -> float:\n",
    "    n = len(true_labels)\n",
    "    loss = 0.0\n",
    "    for i in range(n):\n",
    "        for j in range(len(true_labels[i])):\n",
    "            if true_labels[i][j] == 1:\n",
    "                loss -= math.log(predicted_probs[i][j] + 1e-12)\n",
    "    if loss / n == -0.0:\n",
    "        return 0.0\n",
    "    return loss / n\n",
    "pred = np.array([[1, 0, 0], [0, 1, 0]])\n",
    "true = np.array([[1, 0, 0], [0, 1, 0]])\n",
    "print(compute_cross_entropy_loss(pred, true))\n",
    "print(round(compute_cross_entropy_loss(pred, true), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2de4dc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "[5.5 6.5 7.5]\n",
      "6.5\n",
      "[[4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
    "print(x.shape)\n",
    "print(np.mean(x, axis=(0,1)))\n",
    "print(np.mean(x))\n",
    "print(np.mean(x, axis=(0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
